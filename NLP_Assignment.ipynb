{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8yNrSs8Wu_II",
        "hDrog6MKxaM5",
        "dx83G-zWesjS",
        "r2m-_Zh3GhvX",
        "n68JUuKaG_gu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "Llk68GTsACT_",
        "outputId": "be6ac564-321a-4df3-f7f6-8b0c8cd3288b"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
        "from colab_pdf import colab_pdf\n",
        "colab_pdf('NLP_Assignment_Unit_4.ipynb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-25 09:30:39--  https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864 (1.8K) [text/plain]\n",
            "Saving to: ‘colab_pdf.py’\n",
            "\n",
            "\rcolab_pdf.py          0%[                    ]       0  --.-KB/s               \rcolab_pdf.py        100%[===================>]   1.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-25 09:30:39 (33.9 MB/s) - ‘colab_pdf.py’ saved [1864/1864]\n",
            "\n",
            "Mounted at /content/drive/\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Extracting templates from packages: 100%\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/NLP_Assignment_Unit_4.ipynb to pdf\n",
            "[NbConvertApp] Writing 66987 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 63936 bytes to /content/drive/My Drive/NLP_Assignment_Unit_4.pdf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_60ea4595-62c1-49af-9e37-42fad1df77db\", \"NLP_Assignment_Unit_4.pdf\", 63936)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File ready to be Downloaded and Saved to Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2m-_Zh3GhvX"
      },
      "source": [
        "# Relation of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu32UVeX4r90",
        "outputId": "4479db73-f4de-478d-f952-01320785cf0b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQEJCfe64RrH",
        "outputId": "d3c379d5-cbe0-4ebb-c2d4-4f1c59b13168"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "word1 = \"parking\"\n",
        "word2 = \"building\"\n",
        "\n",
        "syns1 = wordnet.synsets(word1)\n",
        "syns2 = wordnet.synsets(word2)\n",
        "for s1 in syns1:\n",
        "  for s2 in syns2:\n",
        "    lch = s2.lowest_common_hypernyms(s1)\n",
        "    if len(lch) > 0:\n",
        "      print(s1, '<-->', s2, '===', lch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('parking.n.01') <--> Synset('building.n.01') === [Synset('entity.n.01')]\n",
            "Synset('parking.n.01') <--> Synset('construction.n.01') === [Synset('abstraction.n.06')]\n",
            "Synset('parking.n.01') <--> Synset('construction.n.07') === [Synset('abstraction.n.06')]\n",
            "Synset('parking.n.01') <--> Synset('building.n.04') === [Synset('abstraction.n.06')]\n",
            "Synset('parking.n.02') <--> Synset('building.n.01') === [Synset('entity.n.01')]\n",
            "Synset('parking.n.02') <--> Synset('construction.n.01') === [Synset('act.n.02')]\n",
            "Synset('parking.n.02') <--> Synset('construction.n.07') === [Synset('act.n.02')]\n",
            "Synset('parking.n.02') <--> Synset('building.n.04') === [Synset('abstraction.n.06')]\n",
            "Synset('park.v.02') <--> Synset('build.v.05') === [Synset('control.v.01')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n68JUuKaG_gu"
      },
      "source": [
        "# Senses of words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6l8XkQlJJGf",
        "outputId": "b8337594-2f9f-490a-9ff6-5e7bc49871a7"
      },
      "source": [
        "s =  wordnet.synsets('piece')\n",
        "print(\"There are \",len(s), \"senses for the word \\\"piece\\\"\")\n",
        "print()\n",
        "for i in s:\n",
        "  print(i)\n",
        "  print(i.lemmas()[0].name(),':',i.definition())\n",
        "  print(\"Example:\")\n",
        "  print(i.examples())\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are  18 senses for the word \"piece\"\n",
            "\n",
            "Synset('piece.n.01')\n",
            "piece : a separate part of a whole\n",
            "Example:\n",
            "['an important piece of the evidence']\n",
            "\n",
            "Synset('piece.n.02')\n",
            "piece : an item that is an instance of some type; \n",
            "Example:\n",
            "['he designed a new piece of equipment', 'she bought a lovely piece of china']\n",
            "\n",
            "Synset('part.n.03')\n",
            "part : a portion of a natural object\n",
            "Example:\n",
            "['they analyzed the river into three parts', 'he needed a piece of granite']\n",
            "\n",
            "Synset('musical_composition.n.01')\n",
            "musical_composition : a musical work that has been created\n",
            "Example:\n",
            "['the composition is written in four movements']\n",
            "\n",
            "Synset('piece.n.05')\n",
            "piece : an instance of some kind\n",
            "Example:\n",
            "['it was a nice piece of work', 'he had a bit of good luck']\n",
            "\n",
            "Synset('piece.n.06')\n",
            "piece : an artistic or literary composition\n",
            "Example:\n",
            "['he wrote an interesting piece on Iran', 'the children acted out a comic piece to amuse the guests']\n",
            "\n",
            "Synset('firearm.n.01')\n",
            "firearm : a portable gun\n",
            "Example:\n",
            "['he wore his firearm in a shoulder holster']\n",
            "\n",
            "Synset('piece.n.08')\n",
            "piece : a serving that has been cut from a larger portion\n",
            "Example:\n",
            "['a piece of pie', 'a slice of bread']\n",
            "\n",
            "Synset('piece.n.09')\n",
            "piece : a distance\n",
            "Example:\n",
            "['it is down the road a piece']\n",
            "\n",
            "Synset('objet_d'art.n.01')\n",
            "objet_d'art : a work of art of some artistic value\n",
            "Example:\n",
            "[\"this store sells only objets d'art\", 'it is not known who created this piece']\n",
            "\n",
            "Synset('while.n.01')\n",
            "while : a period of indeterminate length (usually short) marked by some action or condition\n",
            "Example:\n",
            "['he was here for a little while', 'I need to rest for a piece', 'a spell of good weather', 'a patch of bad weather']\n",
            "\n",
            "Synset('slice.n.01')\n",
            "slice : a share of something\n",
            "Example:\n",
            "[\"a slice of the company's revenue\"]\n",
            "\n",
            "Synset('man.n.10')\n",
            "man : game equipment consisting of an object used in playing certain board games\n",
            "Example:\n",
            "['he taught me to set up the men on the chess board', 'he sacrificed a piece to get a strategic advantage']\n",
            "\n",
            "Synset('patch.v.01')\n",
            "patch : to join or unite the pieces of\n",
            "Example:\n",
            "['patch the skirt']\n",
            "\n",
            "Synset('assemble.v.01')\n",
            "assemble : create by putting components or members together\n",
            "Example:\n",
            "['She pieced a quilt', 'He tacked together some verses', 'They set up a committee']\n",
            "\n",
            "Synset('piece.v.03')\n",
            "piece : join during spinning\n",
            "Example:\n",
            "['piece the broken pieces of thread, slivers, and rovings']\n",
            "\n",
            "Synset('nibble.v.03')\n",
            "nibble : eat intermittently; take small bites of\n",
            "Example:\n",
            "['He pieced at the sandwich all morning', 'She never eats a full meal--she just nibbles']\n",
            "\n",
            "Synset('piece.v.05')\n",
            "piece : repair by adding pieces\n",
            "Example:\n",
            "['She pieced the china cup']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQmdC95kHDP6"
      },
      "source": [
        "# Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCAVtDAgUXmA"
      },
      "source": [
        "from nltk.corpus import wordnet_ic\n",
        "\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "-eQyBUJCW3f3",
        "outputId": "a3f715c1-5dec-40d9-d33f-7e05c5838a20"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
        "from colab_pdf import colab_pdf\n",
        "colab_pdf('Assignment_3.ipynb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-13 05:34:49--  https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864 (1.8K) [text/plain]\n",
            "Saving to: ‘colab_pdf.py’\n",
            "\n",
            "\rcolab_pdf.py          0%[                    ]       0  --.-KB/s               \rcolab_pdf.py        100%[===================>]   1.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-13 05:34:49 (43.8 MB/s) - ‘colab_pdf.py’ saved [1864/1864]\n",
            "\n",
            "Mounted at /content/drive/\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Extracting templates from packages: 100%\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/Assignment_3.ipynb to pdf\n",
            "[NbConvertApp] Writing 33391 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 29214 bytes to /content/drive/My Drive/Assignment_3.pdf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_48f8169f-c18e-4a9d-9500-e60ed5a54621\", \"Assignment_3.pdf\", 29214)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File ready to be Downloaded and Saved to Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH48tH0sMhVy"
      },
      "source": [
        "word1 = \"bark\"\n",
        "word2 = \"dog\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkd7PjFSMjRF"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        " \n",
        "syn1 = wordnet.synsets(word1)[0]\n",
        "syn2 = wordnet.synsets(word2)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpXYCFeWNB1e"
      },
      "source": [
        "**Wu-Palmer Similarity**\n",
        "\n",
        "Return a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEjnQ1D8NDXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd352e5-6490-4402-8055-7f4fb00c4855"
      },
      "source": [
        "syn1.wup_similarity(syn2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDvpHNVyVJ4u"
      },
      "source": [
        "**Leacock Chodorow Similarity**\n",
        "\n",
        "Return a score denoting how similar two word senses are, based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Wn8xL7TK8h",
        "outputId": "12c92dcf-4ddc-4654-a0a0-6808faa2db6d"
      },
      "source": [
        "syn1.lch_similarity(syn2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4403615823901665"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIzAWrMFVH7a"
      },
      "source": [
        "**Path Distance Similarity**\n",
        "\n",
        "Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGkGYDgSTMYy",
        "outputId": "27d95d6f-3395-4e74-d7e8-f983c29d6458"
      },
      "source": [
        "syn1.path_similarity(syn2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwPNZ-GwU1ym"
      },
      "source": [
        "**Lin Similarity**\n",
        "\n",
        "Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe4lUiKRTIi_",
        "outputId": "731a619b-a0ff-47b1-f9a4-896661253c19"
      },
      "source": [
        "syn1.lin_similarity(syn2, brown_ic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15382740157753266"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4lWu8D1Vgpw"
      },
      "source": [
        "**Jiang-Conrath Similarity**\n",
        "\n",
        "Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVYL6kFgTJtT",
        "outputId": "880cce9c-5c4f-4ca3-e336-8f286ee3df09"
      },
      "source": [
        "syn1.jcn_similarity(syn2, brown_ic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.059338029344951436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB4FHfgOVk3W"
      },
      "source": [
        "**Resnik Similarity**\n",
        "\n",
        "Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ-cuMvCTOQW",
        "outputId": "397160d2-4e9a-4f6b-e5f2-220011cf4db9"
      },
      "source": [
        "syn1.res_similarity(syn2, brown_ic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5318337432196856"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yNrSs8Wu_II"
      },
      "source": [
        "# NLTK Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrvmvIQSO-5u"
      },
      "source": [
        "**Importing nltk**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHJdVTKLOuv2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RafPdekhQT1C"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9GIA3qJd1t3"
      },
      "source": [
        "*Paragraph into sentence*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYC_uAj6d4y7",
        "outputId": "029bbbe8-f6d6-488e-c66b-a59829ae6561"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = 'This is a sample sentence to check the nltk tools.This is a sentence tokenizer. This will split paragraphs into sentences\"\n",
        "\n",
        "token_text = sent_tokenize(text)\n",
        "print(token_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This is a sample sentence to check the nltk tools.This is a sentence tokenizer.', 'This will split paragraphs into sentences']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KONhIZ1mdvHu"
      },
      "source": [
        "*Sentence into words*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9L9noeeQZH_",
        "outputId": "7ced6b46-4fb1-44b3-eab8-6f5e2bafd58b"
      },
      "source": [
        "text = \"This is a sample sentence to check the nltk tools\"\n",
        "token = nltk.word_tokenize(text)\n",
        "print(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', 'to', 'check', 'the', 'nltk', 'tools']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WngO7_t1TPyf"
      },
      "source": [
        "**POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YQNMvbOTSht",
        "outputId": "74670399-e6f4-4c64-a35b-ebc37581f2fe"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "from nltk import pos_tag\n",
        "text = \"This is a sample sentence to check the nltk tools\"\n",
        "t=text.split()\n",
        "tokens_tag = pos_tag(t)\n",
        "print(tokens_tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('to', 'TO'), ('check', 'VB'), ('the', 'DT'), ('nltk', 'NN'), ('tools', 'NNS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr2EdlPMUIjo"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HLyyS4oUOo9"
      },
      "source": [
        "*Porter Stemmer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bINFivjkULFH",
        "outputId": "ab53b557-8246-4645-df7e-5b61ec0bcc0c"
      },
      "source": [
        "from nltk.stem import PorterStemmer  \n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "\n",
        "words = ['cared','university','fairly','easily','singing',\n",
        "       'sings','sung','singer','sportingly'] \n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cared  :  care\n",
            "university  :  univers\n",
            "fairly  :  fairli\n",
            "easily  :  easili\n",
            "singing  :  sing\n",
            "sings  :  sing\n",
            "sung  :  sung\n",
            "singer  :  singer\n",
            "sportingly  :  sportingli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svclfJhxUSTU"
      },
      "source": [
        "*Snowball Stemmer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS2Y23qqUfBe",
        "outputId": "7f246423-5ce8-420d-bd6f-ebca3217a4b1"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "  \n",
        "#the stemmer requires a language parameter\n",
        "snow_stemmer = SnowballStemmer(language='english')\n",
        "  \n",
        "#list of tokenized words\n",
        "words = ['cared','university','fairly','easily','singing',\n",
        "       'sings','sung','singer','sportingly']\n",
        "\n",
        "for w in words:\n",
        "    x = snow_stemmer.stem(w)\n",
        "    print(w+' : '+x)   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cared : care\n",
            "university : univers\n",
            "fairly : fair\n",
            "easily : easili\n",
            "singing : sing\n",
            "sings : sing\n",
            "sung : sung\n",
            "singer : singer\n",
            "sportingly : sport\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT9SchXZPCPd"
      },
      "source": [
        "**N-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la5eviKQPobD",
        "outputId": "7fa6f15f-c297-4ad7-9a0b-751c6a1688f8"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "text = \"This is a sample sentence to check the nltk tools\"\n",
        "unigram = ngrams(token,1)\n",
        "bigrams = ngrams(token,2)\n",
        "trigrams = ngrams(token,3)\n",
        "print(\"Unigram : \")\n",
        "for i in unigram:\n",
        "  print(i,end='')\n",
        "print()\n",
        "print(\"Bigram : \")\n",
        "for i in bigrams:\n",
        "  print(i,end='')\n",
        "print()\n",
        "print(\"Trigram : \")\n",
        "for i in trigrams:\n",
        "  print(i,end='')\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram : \n",
            "('This',)('is',)('a',)('sample',)('sentence',)('to',)('check',)('the',)('nltk',)('tools',)\n",
            "Bigram : \n",
            "('This', 'is')('is', 'a')('a', 'sample')('sample', 'sentence')('sentence', 'to')('to', 'check')('check', 'the')('the', 'nltk')('nltk', 'tools')\n",
            "Trigram : \n",
            "('This', 'is', 'a')('is', 'a', 'sample')('a', 'sample', 'sentence')('sample', 'sentence', 'to')('sentence', 'to', 'check')('to', 'check', 'the')('check', 'the', 'nltk')('the', 'nltk', 'tools')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmnv-9PYV2NO"
      },
      "source": [
        "**Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKap8wYuV6Ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5716a590-f75c-4e81-e519-04b72a9b0098"
      },
      "source": [
        "from nltk.parse import RecursiveDescentParser\n",
        "grammar = nltk.CFG.fromstring(\"\"\"  S -> NP VP\n",
        "  VP -> V NP | V NP PP\n",
        "  PP -> P NP\n",
        "  V -> \"saw\" | \"ate\" | \"walked\"\n",
        "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
        "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        "  \"\"\")\n",
        "\n",
        "rd = RecursiveDescentParser(grammar)\n",
        "sent = \"Mary saw John\".split()\n",
        "for tree in rd.parse(sent):\n",
        "  print(tree)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP Mary) (VP (V saw) (NP John)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDrog6MKxaM5"
      },
      "source": [
        "# Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd7X4cKEQ_Z"
      },
      "source": [
        "**Importing spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5hYYKz9EhRF"
      },
      "source": [
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3PrQ1ldEQ_a"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5C8KAN6EQ_b"
      },
      "source": [
        "*Paragraph into sentence*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqohhBrgg6Zb",
        "outputId": "707caab9-3459-44f4-ec19-c6ed024f8690"
      },
      "source": [
        "document = sp('This is a sample sentence to check the nltk tools.This is a sentence tokenizer.\\\n",
        " This will split paragraphs into sentences')\n",
        "for sentence in document.sents:\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a sample sentence to check the nltk tools.\n",
            "This is a sentence tokenizer.\n",
            "This will split paragraphs into sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2-I2ZOnEQ_c"
      },
      "source": [
        "*Sentence into words*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1D8lpc5hMUh",
        "outputId": "80848e08-ddca-464b-94d5-a56bccd658eb"
      },
      "source": [
        "sentence = sp(u'This is a sample sentence to check the nltk tools')\n",
        "for word in sentence:\n",
        "    print(word.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This\n",
            "is\n",
            "a\n",
            "sample\n",
            "sentence\n",
            "to\n",
            "check\n",
            "the\n",
            "nltk\n",
            "tools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jU5lzn7EQ_c"
      },
      "source": [
        "**POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXm2xdBvhbeQ",
        "outputId": "7e8ae7d1-d641-4be1-a051-9f48c08439a1"
      },
      "source": [
        "sentence = sp(u'This is a sample sentence to check the nltk tools')\n",
        "for word in sentence:\n",
        "    print(word.text, word.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This DET\n",
            "is AUX\n",
            "a DET\n",
            "sample NOUN\n",
            "sentence NOUN\n",
            "to PART\n",
            "check VERB\n",
            "the DET\n",
            "nltk ADJ\n",
            "tools NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_3ECdLgEQ_c"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSLjYLMtiGEo",
        "outputId": "d6d93f12-db01-4ebb-be18-1066709b7c85"
      },
      "source": [
        "sent = sp(u'compute computer computed computing')\n",
        "for word in sent:\n",
        "    print(word.text,\":\",  word.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compute : compute\n",
            "computer : computer\n",
            "computed : compute\n",
            "computing : computing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUqm6cAqEQ_e"
      },
      "source": [
        "**N-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDHsEkd9j5cA",
        "outputId": "17fa48b9-bfb9-46a0-d45f-021ad74c5ed7"
      },
      "source": [
        "sentence = sp(u'This is a sample sentence to check the nltk tool')\n",
        "for i in sentence.noun_chunks:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a sample sentence\n",
            "the nltk tool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmX9ma5EEQ_f"
      },
      "source": [
        "**Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcS4ImekOJxb",
        "outputId": "8a767fe3-aa1d-4f6a-c90c-1fe4f4f3c350"
      },
      "source": [
        "piano_text = 'Gus is learning piano'\n",
        "piano_doc = sp(piano_text)\n",
        "for token in piano_doc:\n",
        "    print (token.text, token.tag_, token.head.text, token.dep_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gus NNP learning nsubj\n",
            "is VBZ learning aux\n",
            "learning VBG learning ROOT\n",
            "piano NN learning dobj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx83G-zWesjS"
      },
      "source": [
        "# Inter Rater Agreement "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIDzrIN4ftr2"
      },
      "source": [
        "!pip install numpy krippendorff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j5kb-0YgSK7",
        "outputId": "9566e53a-d593-46cc-cbc1-1b4fe2bb2d75"
      },
      "source": [
        "import krippendorff\n",
        "data = [[1,3,0,5,5],\n",
        "       [0,3,1,5,2],    # 5 examples\n",
        "       [1,2,0,5,3]]    # 3 raters\n",
        "\n",
        "kappa = krippendorff.alpha(data)\n",
        "print(kappa) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8191214470284238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcWkOoh2hVRW",
        "outputId": "a1396469-dc96-45c9-a5b1-c18d8656a3c6"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "labeler1 = [2, 0, 2, 2, 0, 1]\n",
        "labeler2 = [0, 0, 2, 2, 0, 2]\n",
        "cohen_kappa_score(labeler1, labeler2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4285714285714286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}